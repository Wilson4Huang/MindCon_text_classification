{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9d6084e-b55f-4adf-b03c-f902cb56001a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "import mindspore.nn as nn\n",
    "import mindspore.numpy as mnp\n",
    "from mindspore import Tensor\n",
    "import mindspore.dataset as ds\n",
    "from mindspore.dataset import text\n",
    "from mindspore.dataset.text import JiebaMode\n",
    "from mindspore.dataset.text import NormalizeForm\n",
    "from mindspore import load_checkpoint, load_param_into_net, save_checkpoint\n",
    "from mindspore import set_context, PYNATIVE_MODE\n",
    "import mindspore.ops as ops\n",
    "from mindspore.common.parameter import Parameter\n",
    "from mindspore.common.initializer import Uniform, HeUniform\n",
    "import mindspore\n",
    "import csv\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a435ddfd-957e-4b46-a776-a6804c565d92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://repo.myhuaweicloud.com/repository/pypi/simple\n",
      "Collecting jieba\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/c6/cb/18eeb235f833b726522d7ebed54f2278ce28ba9438e3135ab0278d9792a2/jieba-0.42.1.tar.gz (19.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 19.2 MB 58.7 MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: jieba\n",
      "  Building wheel for jieba (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for jieba: filename=jieba-0.42.1-py3-none-any.whl size=19314476 sha256=85ba2c5b96190c22b6111221ccdf8516a705911f2865c2c80b1e4b4e46d19da3\n",
      "  Stored in directory: /home/ma-user/.cache/pip/wheels/86/bc/97/67c05f24b07573fd425039f944f8bc57c8dbc6f2c0bcc046fa\n",
      "Successfully built jieba\n",
      "Installing collected packages: jieba\n",
      "Successfully installed jieba-0.42.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d86ba953-a436-46da-b555-aadff43d2289",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9419f6e-30dc-42a0-ad8d-be3414f1625d",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_context(mode=PYNATIVE_MODE, device_target='Ascend')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001da65e-facc-46b6-8943-254af31f19fd",
   "metadata": {},
   "source": [
    "在Notebook里面下载自己桶里的数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14c2c91b-58be-4065-8442-d88f1810d317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import moxing as mox\n",
    "# '''\n",
    "# 这个是查询是否数据集创建成功的，里面参数就是 obs:// 然后就是桶里面的路径地址\n",
    "# 成功返回Ture\n",
    "# '''\n",
    "# mox.file.exists('obs://mindcon4wilson/mindcon_text_classification.zip') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b452a2a3-8ddf-4963-95f6-f2c64496ce8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from modelarts.session import Session\n",
    "# session = Session()\n",
    "# session.obs.download_file(src_obs_file=\"obs://mindcon4wilson/mindcon_text_classification.zip\", \n",
    "# dst_local_dir=\"/home/ma-user/work/\")\n",
    "# '''\n",
    "# 第一个参数就是数据集的具体位置，第二个就是你解压的目录，解压在work才会持久化的保存，不然你下次还得下载解压一遍\n",
    "# '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9db2d9a9-bfe0-447b-9508-a755871643a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #解压\n",
    "# !unzip mindcon_text_classification.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d99d9b83-bdfe-4bb5-9fb5-cb21bcf29412",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader():\n",
    "    \"\"\"数据集加载器\n",
    "\n",
    "    加载数据集并处理为一个Python迭代对象。\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, path, type_dataset):\n",
    "        self.path = path\n",
    "        self.datas, self.labels = [], []\n",
    "        self.type_dataset = type_dataset\n",
    "        self._load()\n",
    "\n",
    "    def _load(self):\n",
    "        with open(self.path, \"r\", encoding='utf-8')as file:\n",
    "            txtFile = file.readlines()\n",
    "            if self.type_dataset == \"train\":\n",
    "                for line in txtFile:\n",
    "                    sentence = line.split(\",\")[1].replace(\"\\n\", \"\")\n",
    "                    self.datas.append([word for word in jieba.cut(sentence=sentence, cut_all=True, HMM=True)])\n",
    "                    # label_onehot = [0] * 2\n",
    "                    # label_onehot[int(line.split(\",\")[0])] = 1\n",
    "                    # self.labels.append(label_onehot)\n",
    "                    self.labels.append(int(line.split(\",\")[0]))\n",
    "            if self.type_dataset == \"test\":\n",
    "                for line in txtFile:\n",
    "                    self.datas.append([word for word in jieba.cut(sentence=line, cut_all=True, HMM=True)])\n",
    "                    self.labels.append(0)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.datas[idx], self.labels[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.datas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b1ec524-bdd9-4b32-988b-044fa7f5cade",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(train_data_path, test_data_path):\n",
    "    data_train = ds.GeneratorDataset(DataLoader(path=train_data_path, type_dataset =\"train\"),\n",
    "                                     column_names=[\"review\", \"label\"],\n",
    "                                     shuffle=True)\n",
    "    data_test = ds.GeneratorDataset(DataLoader(path=test_data_path, type_dataset = \"test\"),\n",
    "                                    column_names=[\"review\", \"label\"],\n",
    "                                    shuffle=False)\n",
    "\n",
    "    return data_train, data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87621444-2e98-434c-935d-feabd2bad1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab():\n",
    "    with open(\"/home/ma-user/work/mindcon_text_classification/train/data.txt\", \"r\", encoding = 'utf-8')as file:\n",
    "        txtFile = file.readlines()\n",
    "        tokens = []\n",
    "        index = []\n",
    "        for line in txtFile:\n",
    "            # for word in jieba.cut(sentence=sentence,  cut_all=True, HMM=True):\n",
    "            #     # print(word)\n",
    "            #     if word not in tokens:\n",
    "            #         tokens.append(str(word))\n",
    "            #         index.append(len(tokens))\n",
    "            sentence = line.split(\",\")[1].replace(\"\\n\", \"\")\n",
    "            for word in jieba.cut(sentence=sentence,  cut_all=True, HMM=True):\n",
    "                # print(word)\n",
    "                if word not in tokens:\n",
    "                    tokens.append(str(word))\n",
    "                    index.append(len(tokens))\n",
    "\n",
    "    tokens.append(\"<unk>\")\n",
    "    index.append(len(tokens))\n",
    "    tokens.append(\"<pad>\")\n",
    "    index.append(len(tokens))\n",
    "    vocab_dict = dict(zip(tokens, index))\n",
    "    vocab = text.Vocab.from_dict(word_dict=vocab_dict)\n",
    "    return vocab, len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3f997ee-3506-46d3-b849-47ced75c204f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, train_dataset, epoch=0):\n",
    "    model.set_train()\n",
    "    total = train_dataset.get_dataset_size()\n",
    "    loss_total = 0\n",
    "    step_total = 0\n",
    "    print('******************************training******************************')\n",
    "\n",
    "    with tqdm(total=total) as t:\n",
    "        t.set_description('Epoch %i' % epoch)\n",
    "        for i in train_dataset.create_tuple_iterator():\n",
    "            loss = model(i[0], i[1])\n",
    "            loss_total += loss.asnumpy()\n",
    "            step_total += 1\n",
    "            t.set_postfix(loss=loss_total / step_total)\n",
    "            t.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b6ad799-4337-417c-9e82-febac975f0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y_s):\n",
    "    \"\"\"\n",
    "    计算每个batch的准确率\n",
    "    \"\"\"\n",
    "\n",
    "    # 对预测值进行四舍五入\n",
    "    # print(\"preds:{},ture:{}\".format(preds,y_s))\n",
    "    rounded_preds = np.around(preds)\n",
    "    # print(\"rounded_preds_1:{}\".format(rounded_preds))\n",
    "    # rounded_preds = [np.argmax(i) + 1 for i in rounded_preds]\n",
    "    # print(\"rounded_preds_2:{}\".format(rounded_preds))\n",
    "    # y_s = [np.argmax(i) + 1 for i in y_s]\n",
    "    # print(\"y_s:{}\".format(y_s))\n",
    "    correct = [rounded_pred == y for rounded_pred, y in zip(rounded_preds, y_s)]\n",
    "\n",
    "    acc = correct.count(True) / len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e14701d6-402e-4acb-984b-80115487d971",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_dataset, criterion, epoch=0):\n",
    "    \"\"\"\n",
    "    用验证集进行模型评估\n",
    "    \"\"\"\n",
    "    total = test_dataset.get_dataset_size()\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    step_total = 0\n",
    "    model.set_train(False)\n",
    "    print('******************************evaluting******************************')\n",
    "\n",
    "    with tqdm(total=total) as t:\n",
    "        t.set_description('Epoch %i' % epoch)\n",
    "        for i in test_dataset.create_tuple_iterator():\n",
    "            # print(i[0])\n",
    "            predictions = model(i[0])\n",
    "            loss = criterion(predictions, i[1])\n",
    "            epoch_loss += loss.asnumpy()\n",
    "            acc = binary_accuracy(predictions.asnumpy(), i[1].asnumpy())\n",
    "\n",
    "            epoch_acc += acc\n",
    "\n",
    "            step_total += 1\n",
    "            t.set_postfix(loss=epoch_loss / step_total, acc=epoch_acc / step_total)\n",
    "            t.update(1)\n",
    "        logs_file = open(\"/home/ma-user/work/log_file.txt\", \"a\", encoding='utf-8')\n",
    "        logs_file.write(f'Epoch {epoch} / 30, loss: {epoch_loss / total}, acc: {epoch_acc / total},')\n",
    "        logs_file.close()\n",
    "\n",
    "    return epoch_loss / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "56f940ab-eb9a-40ed-a341-383a8462120b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing(vocab, data_train):\n",
    "    \"\"\"\n",
    "    数据处理、打包\n",
    "    \"\"\"\n",
    "    lookup_op = ds.text.Lookup(vocab, unknown_token='<unk>')\n",
    "    pad_op = ds.transforms.c_transforms.PadEnd([100],\n",
    "                                  pad_value=vocab.tokens_to_ids('<pad>'))\n",
    "    type_cast_op = ds.transforms.c_transforms.TypeCast(mindspore.float32)\n",
    "    tokenizer_op = text.BertTokenizer(vocab=vocab, suffix_indicator='##',\n",
    "                                      max_bytes_per_token=100,\n",
    "                                      unknown_token='<unk>', lower_case=False,\n",
    "                                      keep_whitespace=False,\n",
    "                                      normalization_form=NormalizeForm.NONE,\n",
    "                                      preserve_unused_token=True,\n",
    "                                      with_offsets=False)\n",
    "    # tokenizer_op = text.JiebaTokenizer(\"/home/ma-user/work/hmm_model.utf8\", \"/home/ma-user/work/jieba.dict.utf8\", mode=JiebaMode.MIX, with_offsets=False)\n",
    "    # data_train = data_train.map(operations=[tokenizer_op],\n",
    "    #                             input_columns=['review'])\n",
    "    data_train = data_train.map(operations=[lookup_op, pad_op],\n",
    "                                input_columns=['review'])\n",
    "    data_train = data_train.map(operations=[type_cast_op],\n",
    "                                input_columns=['label'])\n",
    "\n",
    "    data_train, data_valid = data_train.split([0.8, 0.2])\n",
    "\n",
    "    data_train = data_train.batch(100, drop_remainder=True)\n",
    "    data_valid = data_valid.batch(100, drop_remainder=True)\n",
    "\n",
    "    return data_train, data_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e78ef39-8132-4fc4-be1d-4b0b0c11a53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Cell):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers,\n",
    "                 bidirectional, dropout, pad_idx):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim,\n",
    "                                      )\n",
    "        self.rnn = nn.LSTM(embedding_dim,\n",
    "                           hidden_dim,\n",
    "                           num_layers=n_layers,\n",
    "                           bidirectional=bidirectional,\n",
    "                           dropout=dropout,\n",
    "                           batch_first=True)\n",
    "        weight_init = HeUniform(math.sqrt(5))\n",
    "        bias_init = Uniform(1 / math.sqrt(hidden_dim * 2))\n",
    "        self.fc = nn.Dense(hidden_dim * 2, output_dim, weight_init=weight_init, bias_init=bias_init)\n",
    "        self.dropout = nn.Dropout(1-dropout)\n",
    "        self.sigmoid = ops.Sigmoid()\n",
    "        self.squeeze = ops.Squeeze(1)\n",
    "\n",
    "    def construct(self, inputs):\n",
    "        embedded = self.dropout(self.embedding(inputs))\n",
    "        _, (hidden, _) = self.rnn(embedded)\n",
    "        hidden = self.dropout(mnp.concatenate((hidden[-2, :, :], hidden[-1, :, :]), axis=1))\n",
    "        output = self.sigmoid(self.fc(hidden))\n",
    "        output = self.squeeze(output)\n",
    "        # print(output.shape)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cb42f945-08f2-4a1f-be6d-fe5ea0bdde28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Loading model cost 1.481 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "vocab, vocab_len = build_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "50fb338e-3f5f-4e5c-b355-d5ad78a01bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = DataLoader(\"/home/ma-user/work/mindcon_text_classification/train/data.txt\")\n",
    "# data.__getitem__(5550)\n",
    "train_data_path = \"/home/ma-user/work/mindcon_text_classification/train/data.txt\"\n",
    "test_data_path = \"/home/ma-user/work/mindcon_text_classification/test/test.txt\" \n",
    "data_train, data_test = load_data(train_data_path, test_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe905b5a-1514-4cbe-b5db-49f526654579",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(1445:281473275434880,MainProcess):2023-01-15-08:59:32.756.440 [mindspore/dataset/engine/datasets.py:1122] Dataset is shuffled before split.\n"
     ]
    }
   ],
   "source": [
    "data_train, data_valid = data_preprocessing(vocab, data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "92e3d3d6-62c6-4c78-a77a-78196eb6fdfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in data_train.create_tuple_iterator():\n",
    "#     print(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "017a0869-0c05-4ad7-9528-989a5abcd9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_data(net, loss, data_train, data_valid, ckpt_file_name, lr, epochs):\n",
    "    net_with_loss = nn.WithLossCell(net, loss)\n",
    "    optimizer = nn.Adam(net.trainable_params(), learning_rate=lr)\n",
    "    train_one_step = nn.TrainOneStepCell(net_with_loss, optimizer)\n",
    "    best_valid_loss = float('inf')\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        train_one_epoch(train_one_step, data_train, epoch)\n",
    "        valid_loss = evaluate(net, data_valid, loss, epoch)\n",
    "\n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            save_checkpoint(net, ckpt_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cd92c940-a446-4529-a75a-c9bca1e967ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_idx = vocab.tokens_to_ids('<pad>')\n",
    "lr = 0.0001\n",
    "num_layers = 2\n",
    "bidirectional = True\n",
    "output_size = 1\n",
    "hidden_size = 256\n",
    "dropout = 0.5\n",
    "epochs = 30\n",
    "\n",
    "\n",
    "ckpt_file_name = \"/home/ma-user/work/model.ckpt\"\n",
    "\n",
    "\n",
    "loss = nn.BCELoss(reduction='mean')\n",
    "# loss  = nn.FocalLoss()\n",
    "net = RNN(vocab_len, 100, hidden_size, output_size, num_layers, bidirectional, dropout, pad_idx)\n",
    "# training_data(net, loss, data_train, data_valid, ckpt_file_name, lr, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7acb441b-7e3d-4c82-9f0a-05810153f1a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_dict = load_checkpoint(ckpt_file_name)\n",
    "load_param_into_net(net, param_dict)\n",
    "# training_data(net, loss, data_train, data_valid, ckpt_file_name, lr, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "be68411c-dbf6-45a3-87e0-243bc78f71c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(data_test, vocab, ckpt_file_name):\n",
    "    \"\"\"\n",
    "    预测测试集中所有的数据\n",
    "    \"\"\"\n",
    "\n",
    "    # load model\n",
    "    lookup_op = ds.text.Lookup(vocab, unknown_token='<unk>')\n",
    "    pad_op = ds.transforms.c_transforms.PadEnd([100], pad_value=vocab.tokens_to_ids('<pad>'))\n",
    "    # type_cast_op = ds.transforms.c_transforms.TypeCast(mindspore.float32)\n",
    "\n",
    "    # tokenizer_op = text.BertTokenizer(vocab=vocab, suffix_indicator='##',\n",
    "    #                                       max_bytes_per_token=100,\n",
    "    #                                       unknown_token='<unk>', lower_case=False,\n",
    "    #                                       keep_whitespace=False,\n",
    "    #                                       normalization_form=NormalizeForm.NONE,\n",
    "    #                                       preserve_unused_token=True,\n",
    "    #                                       with_offsets=False)\n",
    "\n",
    "#     data_test = data_test.map(operations=[tokenizer_op],\n",
    "#                                     input_columns=['review'])\n",
    "\n",
    "    data_test = data_test.map(operations=[lookup_op],\n",
    "                              input_columns=['review'])\n",
    "    data_test = data_test.map(operations=[pad_op],\n",
    "                              input_columns=['review'])\n",
    "    data_test = data_test.batch(1, drop_remainder=True)\n",
    "\n",
    "    param_dict = load_checkpoint(ckpt_file_name)\n",
    "    load_param_into_net(net, param_dict)\n",
    "\n",
    "    net.set_train(False)\n",
    "    predictions = []\n",
    "\n",
    "    for i in tqdm(data_test.create_tuple_iterator()):\n",
    "        # print(i[0].shape)\n",
    "        # print(type(i[0]))\n",
    "        prediction = net(i[0])\n",
    "        prediction = np.round(prediction.asnumpy())\n",
    "        # prediction = [np.argmax(pred) + 1 for pred in prediction]\n",
    "        # print(prediction[0])\n",
    "        predictions.append(int(prediction[0]))\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "83e519b4-667b-462e-958d-44b4b08d030e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1000it [01:00, 16.45it/s]\n"
     ]
    }
   ],
   "source": [
    "prediction = predict(data_test, vocab, ckpt_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dee280e3-ab93-4814-a7cb-9814d79d5e9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c1c995e1-9083-4806-9e4f-49d35a33f478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"/home/ma-user/work/result2.txt\", 'r', encoding='utf-8') as file:\n",
    "#     prediction2 = file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "90f8d59f-e165-416e-9e5f-c143322f328a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'prediction2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3379/3957401464.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprediction2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'prediction2' is not defined"
     ]
    }
   ],
   "source": [
    "prediction2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c8e2ee-a89c-4913-b95d-cf237ba5ea5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"/home/ma-user/work/result4.txt\", 'w', encoding='utf-8') as file:\n",
    "#     for i in prediction:\n",
    "#         file.write(str(i) + '\\n')\n",
    "#     file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MindSpore",
   "language": "python",
   "name": "mindspore"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
