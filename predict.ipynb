{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95007d1a-8d32-4720-abd6-f1b6d9164c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "import mindspore.nn as nn\n",
    "import mindspore.numpy as mnp\n",
    "from mindspore import Tensor\n",
    "import mindspore.dataset as ds\n",
    "from mindspore.dataset import text\n",
    "from mindspore.dataset.text import JiebaMode\n",
    "from mindspore.dataset.text import NormalizeForm\n",
    "from mindspore import load_checkpoint, load_param_into_net, save_checkpoint\n",
    "from mindspore import set_context, PYNATIVE_MODE\n",
    "import mindspore.ops as ops\n",
    "from mindspore.common.parameter import Parameter\n",
    "from mindspore.common.initializer import Uniform, HeUniform\n",
    "import mindspore\n",
    "import csv\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba493848-f73d-4647-8a76-610ce3d47105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://repo.myhuaweicloud.com/repository/pypi/simple\n",
      "Requirement already satisfied: jieba in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.7/site-packages (0.42.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e18aff83-fed2-436c-aaff-ce116426cb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5703372f-22db-4f3a-ba51-37a5cabf5ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_context(mode=PYNATIVE_MODE, device_target='Ascend')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c329cbb4-848e-44dc-8b21-b94dbeac2baa",
   "metadata": {},
   "source": [
    "构建数据集加载器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c38c202e-8375-4d0a-8aa1-48174c540749",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader():\n",
    "    \"\"\"\n",
    "\n",
    "    加载数据集并处理为一个Python迭代对象。\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, path, type_dataset):\n",
    "        \"\"\"\n",
    "        path:文件路径\n",
    "        type_dataset:数据集类型\n",
    "        \"\"\"\n",
    "        self.path = path\n",
    "        self.datas, self.labels = [], []\n",
    "        self.type_dataset = type_dataset\n",
    "        self._load()\n",
    "\n",
    "    def _load(self):\n",
    "        with open(self.path, \"r\", encoding='utf-8')as file:\n",
    "            txtFile = file.readlines()\n",
    "            if self.type_dataset == \"train\":\n",
    "                for line in txtFile:\n",
    "                    sentence = line.split(\",\")[1].replace(\"\\n\", \"\")\n",
    "                    self.datas.append([word for word in jieba.cut(sentence=sentence, cut_all=True, HMM=True)])\n",
    "                    # label_onehot = [0] * 2\n",
    "                    # label_onehot[int(line.split(\",\")[0])] = 1\n",
    "                    # self.labels.append(label_onehot)\n",
    "                    self.labels.append(int(line.split(\",\")[0]))\n",
    "            if self.type_dataset == \"test\":\n",
    "                for line in txtFile:\n",
    "                    self.datas.append([word for word in jieba.cut(sentence=line, cut_all=True, HMM=True)])\n",
    "                    self.labels.append(0)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.datas[idx], self.labels[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.datas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b512510-8a8d-4eb4-94db-40e4a4eabac8",
   "metadata": {},
   "source": [
    "加载数据集，转化为GeneratorDataset类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2fd250f7-5af9-416b-bdfb-d636114425c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(train_data_path, test_data_path):\n",
    "    data_train = ds.GeneratorDataset(DataLoader(path=train_data_path, type_dataset =\"train\"),\n",
    "                                     column_names=[\"review\", \"label\"],\n",
    "                                     shuffle=True)\n",
    "    data_test = ds.GeneratorDataset(DataLoader(path=test_data_path, type_dataset = \"test\"),\n",
    "                                    column_names=[\"review\", \"label\"],\n",
    "                                    shuffle=False)\n",
    "\n",
    "    return data_train, data_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c7d4f4-ddc8-445d-b21d-2aee378b2722",
   "metadata": {},
   "source": [
    "构建词表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3bdd230b-1b13-40f0-b61c-1c376ae04c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab():\n",
    "    with open(\"/home/ma-user/work/mindcon_text_classification/train/data.txt\", \"r\", encoding = 'utf-8')as file:\n",
    "        txtFile = file.readlines()\n",
    "        tokens = []\n",
    "        index = []\n",
    "        for line in txtFile:\n",
    "            # for word in jieba.cut(sentence=sentence,  cut_all=True, HMM=True):\n",
    "            #     # print(word)\n",
    "            #     if word not in tokens:\n",
    "            #         tokens.append(str(word))\n",
    "            #         index.append(len(tokens))\n",
    "            sentence = line.split(\",\")[1].replace(\"\\n\", \"\")\n",
    "            for word in jieba.cut(sentence=sentence,  cut_all=True, HMM=True):\n",
    "                # print(word)\n",
    "                if word not in tokens:\n",
    "                    tokens.append(str(word))\n",
    "                    index.append(len(tokens))\n",
    "                    \n",
    "    #添加两个特殊token\n",
    "    tokens.append(\"<unk>\")\n",
    "    index.append(len(tokens))\n",
    "    tokens.append(\"<pad>\")\n",
    "    index.append(len(tokens))\n",
    "    vocab_dict = dict(zip(tokens, index))\n",
    "    vocab = text.Vocab.from_dict(word_dict=vocab_dict)\n",
    "    return vocab, len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca6efaad-628d-4850-a52e-000816c9a35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Cell):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers,\n",
    "                 bidirectional, dropout, pad_idx):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim,\n",
    "                                      )\n",
    "        self.rnn = nn.LSTM(embedding_dim,\n",
    "                           hidden_dim,\n",
    "                           num_layers=n_layers,\n",
    "                           bidirectional=bidirectional,\n",
    "                           dropout=dropout,\n",
    "                           batch_first=True)\n",
    "        weight_init = HeUniform(math.sqrt(5))\n",
    "        bias_init = Uniform(1 / math.sqrt(hidden_dim * 2))\n",
    "        self.fc = nn.Dense(hidden_dim * 2, output_dim, weight_init=weight_init, bias_init=bias_init)\n",
    "        self.dropout = nn.Dropout(1-dropout)\n",
    "        self.sigmoid = ops.Sigmoid()\n",
    "        self.squeeze = ops.Squeeze(1)\n",
    "\n",
    "    def construct(self, inputs):\n",
    "        embedded = self.dropout(self.embedding(inputs))\n",
    "        _, (hidden, _) = self.rnn(embedded)\n",
    "        hidden = self.dropout(mnp.concatenate((hidden[-2, :, :], hidden[-1, :, :]), axis=1))\n",
    "        output = self.sigmoid(self.fc(hidden))\n",
    "        output = self.squeeze(output)\n",
    "        # print(output.shape)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "419cc787-7759-4015-a34a-4786aece8a09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 1.354 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "vocab, vocab_len = build_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "234b2c1b-78c9-41b7-bf32-f3ad7b0eaec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = \"/home/ma-user/work/mindcon_text_classification/train/data.txt\"\n",
    "test_data_path = \"/home/ma-user/work/mindcon_text_classification/test/test.txt\" \n",
    "data_train, data_test = load_data(train_data_path, test_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77e8bbd3-e68e-4d27-9906-f7d54e3fbaa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_idx = vocab.tokens_to_ids('<pad>')\n",
    "lr = 0.0001\n",
    "num_layers = 2\n",
    "bidirectional = True\n",
    "output_size = 1\n",
    "hidden_size = 256\n",
    "dropout = 0.5\n",
    "epochs = 30\n",
    "\n",
    "\n",
    "ckpt_file_name = \"/home/ma-user/work/model.ckpt\"\n",
    "\n",
    "net = RNN(vocab_len, 100, hidden_size, output_size, num_layers, bidirectional, dropout, pad_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f4d68c48-a0d9-4e3d-95f7-691891cf47e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(data_test, vocab, ckpt_file_name):\n",
    "    \"\"\"\n",
    "    预测测试集中所有的数据\n",
    "    \"\"\"\n",
    "\n",
    "    # load model\n",
    "    lookup_op = ds.text.Lookup(vocab, unknown_token='<unk>')\n",
    "    pad_op = ds.transforms.c_transforms.PadEnd([100], pad_value=vocab.tokens_to_ids('<pad>'))\n",
    "    # type_cast_op = ds.transforms.c_transforms.TypeCast(mindspore.float32)\n",
    "\n",
    "    # tokenizer_op = text.BertTokenizer(vocab=vocab, suffix_indicator='##',\n",
    "    #                                       max_bytes_per_token=100,\n",
    "    #                                       unknown_token='<unk>', lower_case=False,\n",
    "    #                                       keep_whitespace=False,\n",
    "    #                                       normalization_form=NormalizeForm.NONE,\n",
    "    #                                       preserve_unused_token=True,\n",
    "    #                                       with_offsets=False)\n",
    "\n",
    "#     data_test = data_test.map(operations=[tokenizer_op],\n",
    "#                                     input_columns=['review'])\n",
    "\n",
    "    data_test = data_test.map(operations=[lookup_op],\n",
    "                              input_columns=['review'])\n",
    "    data_test = data_test.map(operations=[pad_op],\n",
    "                              input_columns=['review'])\n",
    "    data_test = data_test.batch(1, drop_remainder=True)\n",
    "\n",
    "    param_dict = load_checkpoint(ckpt_file_name)\n",
    "    load_param_into_net(net, param_dict)\n",
    "\n",
    "    net.set_train(False)\n",
    "    predictions = []\n",
    "\n",
    "    for i in tqdm(data_test.create_tuple_iterator()):\n",
    "        # print(i[0].shape)\n",
    "        # print(type(i[0]))\n",
    "        prediction = net(i[0])\n",
    "        prediction = np.round(prediction.asnumpy())\n",
    "        # prediction = [np.argmax(pred) + 1 for pred in prediction]\n",
    "        # print(prediction[0])\n",
    "        predictions.append(int(prediction[0]))\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1b4bb90-7fee-4610-ba50-33107cca72b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1000it [00:57, 17.36it/s]\n"
     ]
    }
   ],
   "source": [
    "prediction = predict(data_test, vocab, ckpt_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5adee3-5464-45b9-9a65-6680a55a70d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/ma-user/work/result.txt\", 'w', encoding='utf-8') as file:\n",
    "    for i in prediction:\n",
    "        file.write(str(i) + '\\n')\n",
    "    file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MindSpore",
   "language": "python",
   "name": "mindspore"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
