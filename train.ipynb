{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9d6084e-b55f-4adf-b03c-f902cb56001a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "import mindspore.nn as nn\n",
    "import mindspore.numpy as mnp\n",
    "from mindspore import Tensor\n",
    "import mindspore.dataset as ds\n",
    "from mindspore.dataset import text\n",
    "from mindspore.dataset.text import JiebaMode\n",
    "from mindspore.dataset.text import NormalizeForm\n",
    "from mindspore import load_checkpoint, load_param_into_net, save_checkpoint\n",
    "from mindspore import set_context, PYNATIVE_MODE\n",
    "import mindspore.ops as ops\n",
    "from mindspore.common.parameter import Parameter\n",
    "from mindspore.common.initializer import Uniform, HeUniform\n",
    "import mindspore\n",
    "import csv\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a435ddfd-957e-4b46-a776-a6804c565d92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://repo.myhuaweicloud.com/repository/pypi/simple\n",
      "Requirement already satisfied: jieba in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.7/site-packages (0.42.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d86ba953-a436-46da-b555-aadff43d2289",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9419f6e-30dc-42a0-ad8d-be3414f1625d",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_context(mode=PYNATIVE_MODE, device_target='Ascend')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001da65e-facc-46b6-8943-254af31f19fd",
   "metadata": {},
   "source": [
    "在Notebook里面下载自己桶里的数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14c2c91b-58be-4065-8442-d88f1810d317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import moxing as mox\n",
    "# '''\n",
    "# 这个是查询是否数据集创建成功的，里面参数就是 obs:// 然后就是桶里面的路径地址\n",
    "# 成功返回Ture\n",
    "# '''\n",
    "# mox.file.exists('obs://mindcon4wilson/mindcon_text_classification.zip') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b452a2a3-8ddf-4963-95f6-f2c64496ce8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from modelarts.session import Session\n",
    "# session = Session()\n",
    "# session.obs.download_file(src_obs_file=\"obs://mindcon4wilson/mindcon_text_classification.zip\", dst_local_dir=\"/home/ma-user/work/\")\n",
    "# '''\n",
    "# 第一个参数就是数据集的具体位置，第二个就是你解压的目录，解压在work才会持久化的保存，不然你下次还得下载解压一遍\n",
    "# '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9db2d9a9-bfe0-447b-9508-a755871643a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #解压\n",
    "# !unzip mindcon_text_classification.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d99d9b83-bdfe-4bb5-9fb5-cb21bcf29412",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader():\n",
    "    \"\"\"数据集加载器\n",
    "\n",
    "    加载数据集并处理为一个Python迭代对象。\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, path, type_dataset):\n",
    "        self.path = path\n",
    "        self.datas, self.labels = [], []\n",
    "        self.type_dataset = type_dataset\n",
    "        self._load()\n",
    "\n",
    "    def _load(self):\n",
    "        with open(self.path, \"r\", encoding='utf-8')as file:\n",
    "            txtFile = file.readlines()\n",
    "            if self.type_dataset == \"train\":\n",
    "                for line in txtFile:\n",
    "                    sentence = line.split(\",\")[1].replace(\"\\n\", \"\")\n",
    "                    self.datas.append([word for word in jieba.cut(sentence=sentence, cut_all=True, HMM=True)])\n",
    "                    # label_onehot = [0] * 2\n",
    "                    # label_onehot[int(line.split(\",\")[0])] = 1\n",
    "                    # self.labels.append(label_onehot)\n",
    "                    self.labels.append(int(line.split(\",\")[0]))\n",
    "            if self.type_dataset == \"test\":\n",
    "                for line in txtFile:\n",
    "                    self.datas.append([word for word in jieba.cut(sentence=line, cut_all=True, HMM=True)])\n",
    "                    self.labels.append(0)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.datas[idx], self.labels[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.datas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b1ec524-bdd9-4b32-988b-044fa7f5cade",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(train_data_path, test_data_path):\n",
    "    data_train = ds.GeneratorDataset(DataLoader(path=train_data_path, type_dataset =\"train\"),\n",
    "                                     column_names=[\"review\", \"label\"],\n",
    "                                     shuffle=True)\n",
    "    data_test = ds.GeneratorDataset(DataLoader(path=test_data_path, type_dataset = \"test\"),\n",
    "                                    column_names=[\"review\", \"label\"],\n",
    "                                    shuffle=False)\n",
    "\n",
    "    return data_train, data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87621444-2e98-434c-935d-feabd2bad1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab():\n",
    "    with open(\"/home/ma-user/work/mindcon_text_classification/train/data.txt\", \"r\", encoding = 'utf-8')as file:\n",
    "        txtFile = file.readlines()\n",
    "        tokens = []\n",
    "        index = []\n",
    "        for line in txtFile:\n",
    "            # for word in jieba.cut(sentence=sentence,  cut_all=True, HMM=True):\n",
    "            #     # print(word)\n",
    "            #     if word not in tokens:\n",
    "            #         tokens.append(str(word))\n",
    "            #         index.append(len(tokens))\n",
    "            sentence = line.split(\",\")[1].replace(\"\\n\", \"\")\n",
    "            for word in jieba.cut(sentence=sentence,  cut_all=True, HMM=True):\n",
    "                # print(word)\n",
    "                if word not in tokens:\n",
    "                    tokens.append(str(word))\n",
    "                    index.append(len(tokens))\n",
    "\n",
    "    tokens.append(\"<unk>\")\n",
    "    index.append(len(tokens))\n",
    "    tokens.append(\"<pad>\")\n",
    "    index.append(len(tokens))\n",
    "    vocab_dict = dict(zip(tokens, index))\n",
    "    vocab = text.Vocab.from_dict(word_dict=vocab_dict)\n",
    "    return vocab, len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e3f997ee-3506-46d3-b849-47ced75c204f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, train_dataset, epoch=0):\n",
    "    model.set_train()\n",
    "    total = train_dataset.get_dataset_size()\n",
    "    loss_total = 0\n",
    "    step_total = 0\n",
    "    print('******************************training******************************')\n",
    "\n",
    "    with tqdm(total=total) as t:\n",
    "        t.set_description('Epoch %i' % epoch)\n",
    "        for i in train_dataset.create_tuple_iterator():\n",
    "            loss = model(i[0], i[1])\n",
    "            loss_total += loss.asnumpy()\n",
    "            step_total += 1\n",
    "            t.set_postfix(loss=loss_total / step_total)\n",
    "            t.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3b6ad799-4337-417c-9e82-febac975f0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y_s):\n",
    "    \"\"\"\n",
    "    计算每个batch的准确率\n",
    "    \"\"\"\n",
    "\n",
    "    # 对预测值进行四舍五入\n",
    "    rounded_preds = np.around(preds)\n",
    "    correct = [rounded_pred == y for rounded_pred, y in zip(rounded_preds, y_s)]\n",
    "    acc = correct.count(True) / len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e14701d6-402e-4acb-984b-80115487d971",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_dataset, criterion, epoch=0):\n",
    "    \"\"\"\n",
    "    用验证集进行模型评估\n",
    "    \"\"\"\n",
    "    total = test_dataset.get_dataset_size()\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    step_total = 0\n",
    "    model.set_train(False)\n",
    "    print('******************************evaluting******************************')\n",
    "\n",
    "    with tqdm(total=total) as t:\n",
    "        t.set_description('Epoch %i' % epoch)\n",
    "        for i in test_dataset.create_tuple_iterator():\n",
    "            # print(i[0])\n",
    "            predictions = model(i[0])\n",
    "            loss = criterion(predictions, i[1])\n",
    "            epoch_loss += loss.asnumpy()\n",
    "            acc = binary_accuracy(predictions.asnumpy(), i[1].asnumpy())\n",
    "\n",
    "            epoch_acc += acc\n",
    "\n",
    "            step_total += 1\n",
    "            t.set_postfix(loss=epoch_loss / step_total, acc=epoch_acc / step_total)\n",
    "            t.update(1)\n",
    "        #写入训练日志\n",
    "        logs_file = open(\"/home/ma-user/work/log_file.txt\", \"a\", encoding='utf-8')\n",
    "        logs_file.write(f'Epoch {epoch} / 30, loss: {epoch_loss / total}, acc: {epoch_acc / total}' + '\\n')\n",
    "        logs_file.close()\n",
    "\n",
    "    return epoch_loss / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "56f940ab-eb9a-40ed-a341-383a8462120b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing(vocab, data_train):\n",
    "    \"\"\"\n",
    "    数据处理、打包\n",
    "    \"\"\"\n",
    "    lookup_op = ds.text.Lookup(vocab, unknown_token='<unk>')\n",
    "    pad_op = ds.transforms.c_transforms.PadEnd([100],\n",
    "                                  pad_value=vocab.tokens_to_ids('<pad>'))\n",
    "    type_cast_op = ds.transforms.c_transforms.TypeCast(mindspore.float32)\n",
    "    tokenizer_op = text.BertTokenizer(vocab=vocab, suffix_indicator='##',\n",
    "                                      max_bytes_per_token=100,\n",
    "                                      unknown_token='<unk>', lower_case=False,\n",
    "                                      keep_whitespace=False,\n",
    "                                      normalization_form=NormalizeForm.NONE,\n",
    "                                      preserve_unused_token=True,\n",
    "                                      with_offsets=False)\n",
    "    # tokenizer_op = text.JiebaTokenizer(\"/home/ma-user/work/hmm_model.utf8\", \"/home/ma-user/work/jieba.dict.utf8\", mode=JiebaMode.MIX, with_offsets=False)\n",
    "    # data_train = data_train.map(operations=[tokenizer_op],\n",
    "    #                             input_columns=['review'])\n",
    "    data_train = data_train.map(operations=[lookup_op, pad_op],\n",
    "                                input_columns=['review'])\n",
    "    data_train = data_train.map(operations=[type_cast_op],\n",
    "                                input_columns=['label'])\n",
    "\n",
    "    data_train, data_valid = data_train.split([0.8, 0.2])\n",
    "\n",
    "    data_train = data_train.batch(100, drop_remainder=True)\n",
    "    data_valid = data_valid.batch(100, drop_remainder=True)\n",
    "\n",
    "    return data_train, data_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8e78ef39-8132-4fc4-be1d-4b0b0c11a53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Cell):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers,\n",
    "                 bidirectional, dropout, pad_idx):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim,\n",
    "                                      )\n",
    "        self.rnn = nn.LSTM(embedding_dim,\n",
    "                           hidden_dim,\n",
    "                           num_layers=n_layers,\n",
    "                           bidirectional=bidirectional,\n",
    "                           dropout=dropout,\n",
    "                           batch_first=True)\n",
    "        weight_init = HeUniform(math.sqrt(5))\n",
    "        bias_init = Uniform(1 / math.sqrt(hidden_dim * 2))\n",
    "        self.fc = nn.Dense(hidden_dim * 2, output_dim, weight_init=weight_init, bias_init=bias_init)\n",
    "        self.dropout = nn.Dropout(1-dropout)\n",
    "        self.sigmoid = ops.Sigmoid()\n",
    "        self.squeeze = ops.Squeeze(1)\n",
    "\n",
    "    def construct(self, inputs):\n",
    "        embedded = self.dropout(self.embedding(inputs))\n",
    "        _, (hidden, _) = self.rnn(embedded)\n",
    "        hidden = self.dropout(mnp.concatenate((hidden[-2, :, :], hidden[-1, :, :]), axis=1))\n",
    "        output = self.sigmoid(self.fc(hidden))\n",
    "        output = self.squeeze(output)\n",
    "        # print(output.shape)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cb42f945-08f2-4a1f-be6d-fe5ea0bdde28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 1.354 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "vocab, vocab_len = build_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "50fb338e-3f5f-4e5c-b355-d5ad78a01bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = \"/home/ma-user/work/mindcon_text_classification/train/data.txt\"\n",
    "test_data_path = \"/home/ma-user/work/mindcon_text_classification/test/test.txt\" \n",
    "data_train, data_test = load_data(train_data_path, test_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fe905b5a-1514-4cbe-b5db-49f526654579",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(13790:281473222645632,MainProcess):2023-01-15-19:31:12.838.108 [mindspore/dataset/engine/datasets.py:1122] Dataset is shuffled before split.\n"
     ]
    }
   ],
   "source": [
    "data_train, data_valid = data_preprocessing(vocab, data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "017a0869-0c05-4ad7-9528-989a5abcd9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_data(net, loss, data_train, data_valid, ckpt_file_name, lr, epochs):\n",
    "    net_with_loss = nn.WithLossCell(net, loss)\n",
    "    optimizer = nn.Adam(net.trainable_params(), learning_rate=lr)\n",
    "    train_one_step = nn.TrainOneStepCell(net_with_loss, optimizer)\n",
    "    best_valid_loss = float('inf')\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        train_one_epoch(train_one_step, data_train, epoch)\n",
    "        valid_loss = evaluate(net, data_valid, loss, epoch)\n",
    "\n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            save_checkpoint(net, ckpt_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cd92c940-a446-4529-a75a-c9bca1e967ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_idx = vocab.tokens_to_ids('<pad>')\n",
    "lr = 0.0001\n",
    "num_layers = 2\n",
    "bidirectional = True\n",
    "output_size = 1\n",
    "hidden_size = 256\n",
    "dropout = 0.5\n",
    "epochs = 30\n",
    "\n",
    "\n",
    "ckpt_file_name = \"/home/ma-user/work/model.ckpt\"\n",
    "\n",
    "\n",
    "loss = nn.BCELoss(reduction='mean')\n",
    "net = RNN(vocab_len, 100, hidden_size, output_size, num_layers, bidirectional, dropout, pad_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7acb441b-7e3d-4c82-9f0a-05810153f1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data(net, loss, data_train, data_valid, ckpt_file_name, lr, epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MindSpore",
   "language": "python",
   "name": "mindspore"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
